Anna Filippova,
annafil@gmail.com

Connor Gilroy,
cgilroy@uw.edu

Ridhi Kashyap,
ridhi.kashyap@nuffield.ox.ac.uk

Antje Kirchner,
antje.kirchner@unl.edu

Allison Morgan,
allison.morgan@colorado.edu

Kivan Polimis,
kpolimis@uw.edu

Adaner Usmani,
au324@nyu.edu

Tong Wang,
tong-wang@uiowa.edu

Michael Yeomans,
yeomans@fas.harvard.edu

This model is one of a series of models that are part of the same project
by the above team of researchers. What follows is an abbreviated narrative;
for further details, and the full body of code used in the project, please
see the linked GitHub repositories.

# Data and imputation

https://github.com/ccgilroy/ffc-data-processing
https://github.com/annafil/FFCRegressionImputation

The background data are processed as either continuous or categorical variables,
with some limited manual refinement of the classification. Variables with
insufficient variation or too many missing values are dropped entirely.

Missing values are imputed from the most highly correlated variables using a
regression-based imputation strategy. For the set of data used for these
predictions, the model fit to predict missing values is a linear model---OLS
for continuous outcomes, and logistic or multinomial for categorical outcomes.

# Variable selection and priors

https://github.com/formidable-family/collective_wisdom

This model does not use the human-selected variables from the wiki surveys for
variable selection. Instead, it uses all viable variables from the imputed data
set. It does not use score information to relax the regularization on any of
them.

Because it uses so many covariates, this model should have a good in-sample fit
with a low mse, but a poor out-of-sample fit.

# Model

https://github.com/formidable-family/ffc-modeling-pipeline

The model itself is an elasticnet model using the glmnet package. This means
that all covariates are regularized or shrunk toward zero, allowing the model
to make use of more covariates with less overfitting. The degree of
regularization is determined by a lambda parameter tuned through
cross-validation.

Alpha values are closer to 0 than 1, making the model more like ridge regression
than lasso regression. The alpha values were tuned on the training data set
only, using cross-validation; different values of alpha did not appear to
produce particularly different results in terms of out-of-sample mean squared
error.

The code included for submission to the challenge only contains the most
critical and immediate parts of the entire data processing, prior, and
modeling pipeline. To fully reproduce the results, please use the entire code
and directory structure of the linked modeling repository, inserting the data
and variable information from the above sections in the appropriate
subdirectories.
